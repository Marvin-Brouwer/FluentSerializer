name: Continuous Integration `dependabot`

on:
  push:
    branches: [ dependabot/** ]
  pull_request:
    branches: [ dependabot/** ]

concurrency:
  group: ci-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    name: "Build binaries"

    runs-on: ubuntu-latest

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        lfs: true
    ###
    # 🧰 Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          5.0.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🛠 Build
    #
    # Build the library code for later use
    ###
    - name: 🛠 Build
      run: dotnet build --no-restore --nologo --configuration "Release"
    ###
    # 🗃 Publish 'library-binaries' artifacts
    ###
    - name: 🗃 Publish 'library-binaries' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: library-binaries
        path: |
          ./src/**/bin/Release/*/*.*
          !./src/*.Tests
          !./src/*.TestUtils
          !./src/*.Usecase.*
          !./src/*.Benchmark
          !./src/*.BenchmarkUtils
        retention-days: 1
    ###
    # 🗃 Publish 'test-binaries' artifacts
    ###
    - name: 🗃 Publish 'test-binaries' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: test-binaries
        path: |
          ./src/**/obj/Release/*/*.*
          ./src/*.Tests/bin/Release/*/*.*
          ./src/*.Tests/bin/Release/*/Tests/**/*.*
          ./src/*.TestUtils/bin/Release/*/*.*
          ./src/*.Usecase.*/bin/Release/*/*.*
        retention-days: 1
    ###
    # 🗃 Publish 'benchmark-binaries' artifacts
    ###
    - name: 🗃 Publish 'benchmark-binaries' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: benchmark-binaries
        path: |
          ./src/*.Benchmark/bin/Release/*/*.*
          ./src/*/bin/Release/*/ref/*.*
          ./src/*/obj/Release/*/ref/*.*
          ./src/*/bin/Release/*/apphost
          ./src/*/obj/Release/*/apphost
          ./src/*.BenchmarkUtils/bin/Release/*/*.*
          ./src/*.TestUtils/bin/Release/*/*.*
        retention-days: 1
    ###
    # 🗃 Publish 'CodeQL' artifacts
    ###
    - name: 🗃 Publish 'CodeQL' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: github-codeql
        path: |
          /home/runner/work/_temp/codeql_databases/csharp
        retention-days: 1

  test:
    name: "Test"

    runs-on: ubuntu-latest
    needs: build

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # 🧰 Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          5.0.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🗃 Restore 'library-binaries'
    ###
    - name: 🗃 Restore 'library-binaries'
      uses: actions/download-artifact@v2
      with:
        name: library-binaries
        path: ./src
    ###
    # 🗃 Restore 'test-binaries'
    ###
    - name: 🗃 Restore 'test-binaries'
      uses: actions/download-artifact@v2
      with:
        name: test-binaries
        path: ./src
    ###
    # 🗃 Restore 'benchmark-binaries'
    #
    # Because `dotnet test` tries to load all csproj files, we need these dlls.
    ###
    - name: 🗃 Restore 'benchmark-binaries'
      uses: actions/download-artifact@v2
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # 🧪 Run unit tests
    #
    # Run the unit tests of category `UnitTest` and generate a code coverage report.
    ###
    - name: 🧪 Run unit tests
      run: >-
        dotnet test --verbosity:normal
        --no-build --no-restore --nologo
        --configuration="Release"
        --logger:"console;verbosity=detailed"
        --logger:"GitHubActions"
        --results-directory:"${{github.workspace}}/test-results"
        --filter:"Category=UnitTest"
        /clp:forceconsolecolor
        "${{github.workspace}}/FluentSerializer.sln";
      working-directory: ${{github.workspace}}
    ###
    # 🧪 Run integration tests
    #
    # Run the unit tests of category `IntegrationTest` to verify the integration of various unit-tests.
    ###
    - name: 🧪 Run integration tests
      run:  >-
        dotnet test --verbosity:normal
        --no-build --no-restore --nologo
        --configuration="Release"
        --logger:"console;verbosity=detailed"
        --logger:"GitHubActions"
        --filter:"Category=IntegrationTest"
        /clp:forceconsolecolor
        "${{github.workspace}}/FluentSerializer.sln";
      working-directory: ${{github.workspace}}
    ###
    # 🧪 Run use-case tests
    #
    # Run the unit tests of category `UseCase` to illustrate the library still works as intended.
    # This is basically a kind of integration test.
    ###
    - name: 🧪 Run use-case tests
      run:  >-
        dotnet test --verbosity:normal
        --no-build --no-restore --nologo
        --configuration="Release"
        --logger:"console;verbosity=detailed"
        --logger:"GitHubActions"
        --filter:"Category=UseCase"
        /clp:forceconsolecolor
        "${{github.workspace}}/FluentSerializer.sln";
      working-directory: ${{github.workspace}}
    ###
    # 🗃 Cancel after test failure
    # Because the benchmark run runs in parallel with testing we need to cancel that when tests fail.
    ###
    - name: 🗃 Cancel after test failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-xml:
    name: "Benchmark (XML)"

    runs-on: ubuntu-latest
    needs: test

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # 🧰 Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          5.0.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🗃 Restore 'library-binaries'
    ###
    - name: 🗃 Restore 'library-binaries'
      uses: actions/download-artifact@v2
      with:
        name: library-binaries
        path: ./src
    ###
    # 🗃 Restore 'benchmark-binaries'
    ###
    - name: 🗃 Restore 'benchmark-binaries'
      uses: actions/download-artifact@v2
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # ⏱ Benchmark XML serializer
    #
    # Benchmark the code for serializing and deserializing a test set of XML data.
    ###
    - name: ⏱ Benchmark XML serializer
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Xml.Benchmark
        sudo ./net6.0/FluentSerializer.Xml.Benchmark --quick=true
      working-directory: ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release
    ###
    # 🗃 Collect benchmark reports
    ###
    - name: 🗃 Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./xml-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # 🗃 Publish 'benchmark-results' artifacts
    ###
    - name: 🗃 Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: benchmark-results-xml
        path: ${{github.workspace}}/benchmark-results
        retention-days: 1
    ###
    # 🗃 Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: 🗃 Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-json:
    name: "Benchmark (JSON)"

    runs-on: ubuntu-latest
    needs: test

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # 🧰 Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          5.0.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🗃 Restore 'library-binaries'
    ###
    - name: 🗃 Restore 'library-binaries'
      uses: actions/download-artifact@v2
      with:
        name: library-binaries
        path: ./src
    ###
    # 🗃 Restore 'benchmark-binaries'
    ###
    - name: 🗃 Restore 'benchmark-binaries'
      uses: actions/download-artifact@v2
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # ⏱ Benchmark JSON serializer
    #
    # Benchmark the code  for serializing and deserializing a test set of JSON data.
    ###
    - name: ⏱ Benchmark JSON serializer
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Json.Benchmark
        sudo ./net6.0/FluentSerializer.Json.Benchmark --quick=true
      working-directory: ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release
    ###
    # 🗃 Collect benchmark reports
    ###
    - name: 🗃 Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./json-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # 🗃 Publish 'benchmark-results' artifacts
    ###
    - name: 🗃 Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: benchmark-results-json
        path: ${{github.workspace}}/benchmark-results
        retention-days: 1
    ###
    # 🗃 Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: 🗃 Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })
