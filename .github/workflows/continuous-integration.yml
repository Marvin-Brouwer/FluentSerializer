name: Continuous Integration

on:
  push:
    branches:
      - '**'
      - '!dependabot/**'
  pull_request:
    branches:
      - '**'
      - '!main'
      - '!dependabot/**'
  workflow_dispatch:

concurrency:
  group: ci-${{ github.ref }}${{ github.event_name == 'workflow_dispatch' && '[dispatch]' || '' }}
  cancel-in-progress: true

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

jobs:
  build:
    name: "Build"
    uses: ./.github/workflows/continuous-integration.build.yml
    secrets: inherit

  code-ql:
    name: "QA"
    needs: build
    uses: ./.github/workflows/continuous-integration.code-ql.yml
  test:
    name: "QA"
    needs: build
    uses: ./.github/workflows/continuous-integration.test.yml
  sonar-cloud:
    name: "QA"
    needs: [test, code-ql]
    uses: ./.github/workflows/continuous-integration.sonar-cloud.yml
    secrets: inherit

  benchmark-core:
    name: "Benchmark Core libraries"

    runs-on: ubuntu-latest
    needs: [test, code-ql, sonar-cloud]

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # 🧰 Install .Net SDKs
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Install .Net SDKs
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🗃 Restore 'library-binaries'
    ###
    - name: 🗃 Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # 🗃 Restore 'benchmark-binaries'
    ###
    - name: 🗃 Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # ⏱ FluentSerializer.Core
    #
    # Benchmark the code in the core library
    ###
    - name: ⏱ Benchmark FluentSerializer.Core
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Core.Benchmark
        sudo ./net6.0/FluentSerializer.Core.Benchmark --jobType=Long
        sudo ./netcoreapp3.1/FluentSerializer.Core.Benchmark --jobType=Long
      working-directory: ${{github.workspace}}/src/FluentSerializer.Core.Benchmark/bin/Release
    ###
    # 🗃 Collect benchmark reports
    ###
    - name: 🗃 Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Core.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./core-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # 📈 Report benchmarks
    #
    # Publish benchmark results to job overview
    ###
    - name: 📈 Report benchmarks
      run: |-
        cat ./core-benchmark-netcoreapp_6_0-github.md >>$GITHUB_STEP_SUMMARY
        cat ./core-benchmark-netcoreapp_3_1-github.md >>$GITHUB_STEP_SUMMARY
      working-directory: ${{github.workspace}}/src/FluentSerializer.Core.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/
    ###
    # 🗃 Publish 'benchmark-results' artifacts
    ###
    - name: 🗃 Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-core
        path: ${{github.workspace}}/benchmark-results
        retention-days: 20
    ###
    # 🗃 Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: 🗃 Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-xml:
    name: "Benchmark XML"

    runs-on: ubuntu-latest
    needs: [test, code-ql, sonar-cloud]

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # 🧰 Install .Net SDKs
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Install .Net SDKs
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🗃 Restore 'library-binaries'
    ###
    - name: 🗃 Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # 🗃 Restore 'benchmark-binaries'
    ###
    - name: 🗃 Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # ⏱ Benchmark XML serializer
    #
    # Benchmark the code for serializing and deserializing a test set of XML data.
    ###
    - name: ⏱ Benchmark XML serializer
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Xml.Benchmark
        sudo ./net6.0/FluentSerializer.Xml.Benchmark --jobType=Default
        sudo ./netcoreapp3.1/FluentSerializer.Xml.Benchmark --no-generate --jobType=Default
      working-directory: ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release
    ###
    # 🗃 Collect benchmark reports
    ###
    - name: 🗃 Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./xml-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # 📈 Report benchmarks
    #
    # Publish benchmark results to job overview
    ###
    - name: 📈 Report benchmarks
      run: |-
        cat ./xml-serializer-benchmark-netcoreapp_6_0-github.md >>$GITHUB_STEP_SUMMARY
        cat ./xml-serializer-benchmark-netcoreapp_3_1-github.md >>$GITHUB_STEP_SUMMARY
      working-directory: ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/
    ###
    # 🗃 Publish 'benchmark-results' artifacts
    ###
    - name: 🗃 Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-xml
        path: ${{github.workspace}}/benchmark-results
        retention-days: 20
    ###
    # 🗃 Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: 🗃 Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-json:
    name: "Benchmark JSON"

    runs-on: ubuntu-latest
    needs: [test, code-ql, sonar-cloud]

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # 🧰 Install .Net SDKs
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: 🧰 Install .Net SDKs
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # 🗃 Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: 🗃 Restore dependencies
      run: dotnet restore
    ###
    # 🗃 Restore 'library-binaries'
    ###
    - name: 🗃 Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # 🗃 Restore 'benchmark-binaries'
    ###
    - name: 🗃 Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # ⏱ Benchmark JSON serializer
    #
    # Benchmark the code  for serializing and deserializing a test set of JSON data.
    ###
    - name: ⏱ Benchmark JSON serializer
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Json.Benchmark
        sudo ./net6.0/FluentSerializer.Json.Benchmark --jobType=Default
        sudo ./netcoreapp3.1/FluentSerializer.Json.Benchmark --no-generate --jobType=Default
      working-directory: ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release
    ###
    # 🗃 Collect benchmark reports
    ###
    - name: 🗃 Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./json-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # 📈 Report benchmarks
    #
    # Publish benchmark results to job overview
    ###
    - name: 📈 Report benchmarks
      run: |-
        cat ./json-serializer-benchmark-netcoreapp_6_0-github.md >>$GITHUB_STEP_SUMMARY
        cat ./json-serializer-benchmark-netcoreapp_3_1-github.md >>$GITHUB_STEP_SUMMARY
      working-directory: ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/
    ###
    # 🗃 Publish 'benchmark-results' artifacts
    ###
    - name: 🗃 Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-json
        path: ${{github.workspace}}/benchmark-results
        retention-days: 20
    ###
    # 🗃 Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: 🗃 Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })
