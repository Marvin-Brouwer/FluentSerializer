name: Continuous Integration

on:
  push:
    branches:
      - '**'
      - '!dependabot/**'
  pull_request:
    branches:
      - '**'
      - '!main'
      - '!dependabot/**'
  workflow_dispatch:

concurrency:
  group: ci-${{ github.ref }}${{ github.event_name == 'workflow_dispatch' && '[dispatch]' || '' }}
  cancel-in-progress: true

jobs:
  build:
    name: "Build binaries"

    runs-on: ubuntu-latest

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        lfs: true
    ###
    # ðŸ§° Initialize CodeQL
    #
    # Initializes the CodeQL tools for scanning.
    ###
    - name: ðŸ§° Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        config-file: '${{github.workspace}}/.github/codeql/codeql-config.yml'
        languages: 'csharp'
    ###
    # ðŸ§° Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: ðŸ§° Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # ðŸ—ƒ Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: ðŸ—ƒ Restore dependencies
      run: dotnet restore
    ###
    # ðŸ›  Build
    #
    # Build the library code for later use
    ###
    - name: ðŸ›  Build
      run: dotnet build --no-restore --nologo --configuration "Release"
    ###
    # ðŸ—ƒ Publish 'library-binaries' artifacts
    ###
    - name: ðŸ—ƒ Publish 'library-binaries' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: library-binaries
        path: |
          ./src/**/bin/Release/*/*.*
          !./src/*.Tests
          !./src/*.TestUtils
          !./src/*.Usecase.*
          !./src/*.Benchmark
          !./src/*.BenchmarkUtils
        retention-days: 1
    ###
    # ðŸ—ƒ Publish 'test-binaries' artifacts
    ###
    - name: ðŸ—ƒ Publish 'test-binaries' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: test-binaries
        path: |
          ./src/**/obj/Release/*/*.*
          ./src/*.Tests/bin/Release/*/*.*
          ./src/*.Tests/bin/Release/*/Tests/**/*.*
          ./src/*.TestUtils/bin/Release/*/*.*
          ./src/*.Usecase.*/bin/Release/*/*.*
        retention-days: 1
    ###
    # ðŸ—ƒ Publish 'benchmark-binaries' artifacts
    ###
    - name: ðŸ—ƒ Publish 'benchmark-binaries' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-binaries
        path: |
          ./src/*.Benchmark/bin/Release/*/*.*
          ./src/*/bin/Release/*/ref/*.*
          ./src/*/obj/Release/*/ref/*.*
          ./src/*/bin/Release/*/apphost
          ./src/*/obj/Release/*/apphost
          ./src/*.BenchmarkUtils/bin/Release/*/*.*
          ./src/*.TestUtils/bin/Release/*/*.*
        retention-days: 1
    ###
    # ðŸ—ƒ Publish 'CodeQL' artifacts
    ###
    - name: ðŸ—ƒ Publish 'CodeQL' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: github-codeql
        path: |
          /home/runner/work/_temp/codeql_databases/csharp
        retention-days: 1

  quality-check:
    name: "Static Quality Analyses"

    runs-on: ubuntu-latest
    needs: build

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # ðŸ§° Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: ðŸ§° Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # ðŸ§° Initialize CodeQL
    #
    # Initializes the CodeQL tools for scanning.
    ###
    - name: ðŸ§° Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        config-file: '${{github.workspace}}/.github/codeql/codeql-config.yml'
        languages: 'csharp'
    ###
    # ðŸ—ƒ Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: ðŸ—ƒ Restore dependencies
      run: dotnet restore
    ###
    # ðŸ—ƒ Restore 'library-binaries'
    ###
    - name: ðŸ—ƒ Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # ðŸ—ƒ Restore 'github-codeql'
    ###
    - name: ðŸ—ƒ Restore 'github-codeql'
      uses: actions/download-artifact@v3
      with:
        name: github-codeql
        path: /home/runner/work/_temp/codeql_databases/csharp
    ###
    # ðŸ”¬ Perform CodeQL Analysis
    ###
    - name: ðŸ”¬ Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2
    ###
    # ðŸ—ƒ Cancel after test failure
    # Because the benchmark run runs in parallel with testing we need to cancel that when tests fail.
    ###
    - name: ðŸ—ƒ Cancel after test failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  test:
    name: "Automated testing"

    runs-on: ubuntu-latest
    needs: build

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # ðŸ§° Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: ðŸ§° Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # ðŸ§° Setup .Net tools
    #
    # Register tools necessary to run the `test` pipeline-job
    ###
    - name: ðŸ§° Setup .NET tools
      run: |-
        dotnet tool install --global dotnet-reportgenerator-globaltool
        dotnet tool install --global dotnet-coverage
        dotnet tool install --global coverlet.console
    ###
    # ðŸ—ƒ Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: ðŸ—ƒ Restore dependencies
      run: dotnet restore
    ###
    # ðŸ—ƒ Restore 'library-binaries'
    ###
    - name: ðŸ—ƒ Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # ðŸ—ƒ Restore 'test-binaries'
    ###
    - name: ðŸ—ƒ Restore 'test-binaries'
      uses: actions/download-artifact@v3
      with:
        name: test-binaries
        path: ./src
    ###
    # ðŸ—ƒ Restore 'benchmark-binaries'
    #
    # Because `dotnet test` tries to load all csproj files, we need these dlls.
    ###
    - name: ðŸ—ƒ Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # ðŸ§ª Run unit tests
    #
    # Run the unit tests of category `UnitTest` and generate a code coverage report.
    ###
    - name: ðŸ§ª Run unit tests
      run: >-
        dotnet test --verbosity:normal
        --no-build --no-restore --nologo
        --configuration="Release"
        --logger:"console;verbosity=detailed"
        --logger:"GitHubActions"
        --logger "trx;LogFileName=test-results.trx"
        --collect "DotnetCodeCoverage"
        --collect "XPlat Code coverage"
        --results-directory:"${{github.workspace}}/test-results"
        --filter:"Category=UnitTest"
        /p:CollectCoverage="true"
        /p:CoverletOutputFormat="opencover"
        /p:CoverletOutput="${{github.workspace}}/test-results/coverage/"
        /p:MergeWith="${{github.workspace}}/test-results/coverage/"
        /p:Exclude="[*Tests]*%2c[*TestUtils]*%2c[*UseCase*]*"
        /clp:forceconsolecolor
        "${{github.workspace}}/FluentSerializer.sln";

        reportgenerator
        -reports:"${{github.workspace}}/test-results/*/coverage.cobertura.xml"
        -targetdir:"${{github.workspace}}/test-results/coverage"
        -reporttypes:HtmlInline_AzurePipelines\;Cobertura;
      working-directory: ${{github.workspace}}
    ###
    # ðŸ§ª Run integration tests
    #
    # Run the unit tests of category `IntegrationTest` to verify the integration of various unit-tests.
    ###
    - name: ðŸ§ª Run integration tests
      run: >-
        dotnet test --verbosity:normal
        --no-build --no-restore --nologo
        --configuration="Release"
        --logger:"console;verbosity=detailed"
        --logger:"GitHubActions"
        --filter:"Category=IntegrationTest"
        /clp:forceconsolecolor
        "${{github.workspace}}/FluentSerializer.sln";
      working-directory: ${{github.workspace}}
    ###
    # ðŸ§ª Run use-case tests
    #
    # Run the unit tests of category `UseCase` to illustrate the library still works as intended.
    # This is basically a kind of integration test.
    ###
    - name: ðŸ§ª Run use-case tests
      run: >-
        dotnet test --verbosity:normal
        --no-build --no-restore --nologo
        --configuration="Release"
        --logger:"console;verbosity=detailed"
        --logger:"GitHubActions"
        --filter:"Category=UseCase"
        /clp:forceconsolecolor
        "${{github.workspace}}/FluentSerializer.sln";
      working-directory: ${{github.workspace}}
    ###
    # ðŸ—ƒ Publish 'unit-test-coverage' artifacts
    ###
    - name: ðŸ—ƒ Publish 'unit-test-coverage' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: unit-test-coverage
        path: ./test-results/coverage
        retention-days: 1
    ###
    # ðŸ—ƒ Cancel after test failure
    # Because the benchmark run runs in parallel with testing we need to cancel that when tests fail.
    ###
    - name: ðŸ—ƒ Cancel after test failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-core:
    name: "Benchmark Core libraries"

    runs-on: ubuntu-latest
    needs: [test, quality-check]

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # ðŸ§° Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: ðŸ§° Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # ðŸ—ƒ Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: ðŸ—ƒ Restore dependencies
      run: dotnet restore
    ###
    # ðŸ—ƒ Restore 'library-binaries'
    ###
    - name: ðŸ—ƒ Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # ðŸ—ƒ Restore 'benchmark-binaries'
    ###
    - name: ðŸ—ƒ Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # â± FluentSerializer.Core
    #
    # Benchmark the code in the core library
    ###
    - name: â± Benchmark FluentSerializer.Core
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Core.Benchmark
        sudo ./net6.0/FluentSerializer.Core.Benchmark --jobType=Long
        sudo ./netcoreapp3.1/FluentSerializer.Core.Benchmark --jobType=Long
      working-directory: ${{github.workspace}}/src/FluentSerializer.Core.Benchmark/bin/Release
    ###
    # ðŸ—ƒ Collect benchmark reports
    ###
    - name: ðŸ—ƒ Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Core.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./core-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # ðŸ“ˆ Report benchmarks
    #
    # Publish benchmark results to job overview
    ###
    - name: ðŸ“ˆ Report benchmarks
      run: |-
        cat ./core-benchmark-netcoreapp_6_0-github.md >>$GITHUB_STEP_SUMMARY
        cat ./core-benchmark-netcoreapp_3_1-github.md >>$GITHUB_STEP_SUMMARY
      working-directory: ${{github.workspace}}/src/FluentSerializer.Core.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/
    ###
    # ðŸ—ƒ Publish 'benchmark-results' artifacts
    ###
    - name: ðŸ—ƒ Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-core
        path: ${{github.workspace}}/benchmark-results
        retention-days: 20
    ###
    # ðŸ—ƒ Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: ðŸ—ƒ Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-xml:
    name: "Benchmark XML"

    runs-on: ubuntu-latest
    needs: [test, quality-check]

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # ðŸ§° Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: ðŸ§° Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # ðŸ—ƒ Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: ðŸ—ƒ Restore dependencies
      run: dotnet restore
    ###
    # ðŸ—ƒ Restore 'library-binaries'
    ###
    - name: ðŸ—ƒ Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # ðŸ—ƒ Restore 'benchmark-binaries'
    ###
    - name: ðŸ—ƒ Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # â± Benchmark XML serializer
    #
    # Benchmark the code for serializing and deserializing a test set of XML data.
    ###
    - name: â± Benchmark XML serializer
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Xml.Benchmark
        sudo ./net6.0/FluentSerializer.Xml.Benchmark --jobType=Default
        sudo ./netcoreapp3.1/FluentSerializer.Xml.Benchmark --no-generate --jobType=Default
      working-directory: ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release
    ###
    # ðŸ—ƒ Collect benchmark reports
    ###
    - name: ðŸ—ƒ Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./xml-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # ðŸ“ˆ Report benchmarks
    #
    # Publish benchmark results to job overview
    ###
    - name: ðŸ“ˆ Report benchmarks
      run: |-
        cat ./xml-serializer-benchmark-netcoreapp_6_0-github.md >>$GITHUB_STEP_SUMMARY
        cat ./xml-serializer-benchmark-netcoreapp_3_1-github.md >>$GITHUB_STEP_SUMMARY
      working-directory: ${{github.workspace}}/src/FluentSerializer.Xml.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/
    ###
    # ðŸ—ƒ Publish 'benchmark-results' artifacts
    ###
    - name: ðŸ—ƒ Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-xml
        path: ${{github.workspace}}/benchmark-results
        retention-days: 20
    ###
    # ðŸ—ƒ Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: ðŸ—ƒ Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })

  benchmark-json:
    name: "Benchmark JSON"

    runs-on: ubuntu-latest
    needs: [test, quality-check]

    steps:
    ###
    # Checkout repository
    ###
    - name: Checkout repository
      uses: actions/checkout@v3
    ###
    # ðŸ§° Setup .Net
    #
    # Configure the pipeline to use the correct .Net sdk versions
    ###
    - name: ðŸ§° Setup .NET
      uses: actions/setup-dotnet@v2
      with:
        include-prerelease: true
        dotnet-version: |
          3.1.x
          6.0.x
    ###
    # ðŸ—ƒ Restore dependencies
    #
    # Fill the NuGet store with necessary libraries
    ###
    - name: ðŸ—ƒ Restore dependencies
      run: dotnet restore
    ###
    # ðŸ—ƒ Restore 'library-binaries'
    ###
    - name: ðŸ—ƒ Restore 'library-binaries'
      uses: actions/download-artifact@v3
      with:
        name: library-binaries
        path: ./src
    ###
    # ðŸ—ƒ Restore 'benchmark-binaries'
    ###
    - name: ðŸ—ƒ Restore 'benchmark-binaries'
      uses: actions/download-artifact@v3
      with:
        name: benchmark-binaries
        path: ./src
    ###
    # â± Benchmark JSON serializer
    #
    # Benchmark the code  for serializing and deserializing a test set of JSON data.
    ###
    - name: â± Benchmark JSON serializer
      run: |-
        sudo chmod +rwx ./*/FluentSerializer.Json.Benchmark
        sudo ./net6.0/FluentSerializer.Json.Benchmark --jobType=Default
        sudo ./netcoreapp3.1/FluentSerializer.Json.Benchmark --no-generate --jobType=Default
      working-directory: ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release
    ###
    # ðŸ—ƒ Collect benchmark reports
    ###
    - name: ðŸ—ƒ Collect benchmark reports
      if: ${{ always() }}
      run: |-
        sudo mkdir ${{github.workspace}}/benchmark-results
        cd ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/results
        sudo mv ./json-*.md ${{github.workspace}}/benchmark-results/
      working-directory: ${{github.workspace}}
    ###
    # ðŸ“ˆ Report benchmarks
    #
    # Publish benchmark results to job overview
    ###
    - name: ðŸ“ˆ Report benchmarks
      run: |-
        cat ./json-serializer-benchmark-netcoreapp_6_0-github.md >>$GITHUB_STEP_SUMMARY
        cat ./json-serializer-benchmark-netcoreapp_3_1-github.md >>$GITHUB_STEP_SUMMARY
      working-directory: ${{github.workspace}}/src/FluentSerializer.Json.Benchmark/bin/Release/BenchmarkDotNet.Artifacts/
    ###
    # ðŸ—ƒ Publish 'benchmark-results' artifacts
    ###
    - name: ðŸ—ƒ Publish 'benchmark-results' artifacts
      if: ${{ always() }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-json
        path: ${{github.workspace}}/benchmark-results
        retention-days: 20
    ###
    # ðŸ—ƒ Cancel after benchmark failure
    # Because the tests run runs in parallel with testing we need to cancel that when benchmarking fails.
    ###
    - name: ðŸ—ƒ Cancel after benchmark failure
      if: ${{ failure() }}
      uses: actions/github-script@v6
      with:
          script: |
              github.actions.cancelWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: context.runId
              })
